{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:53.57052Z","iopub.execute_input":"2021-12-07T05:38:53.570892Z","iopub.status.idle":"2021-12-07T05:38:53.879702Z","shell.execute_reply.started":"2021-12-07T05:38:53.570852Z","shell.execute_reply":"2021-12-07T05:38:53.878948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:53.881538Z","iopub.execute_input":"2021-12-07T05:38:53.881843Z","iopub.status.idle":"2021-12-07T05:38:54.417696Z","shell.execute_reply.started":"2021-12-07T05:38:53.881795Z","shell.execute_reply":"2021-12-07T05:38:54.417004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading The Dataset","metadata":{}},{"cell_type":"code","source":"lines = open('../input/chatbot-data/cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8',\n             errors='ignore').read().split('\\n')\n\nconvers = open('../input/chatbot-data/cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8',\n             errors='ignore').read().split('\\n')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:54.418899Z","iopub.execute_input":"2021-12-07T05:38:54.419237Z","iopub.status.idle":"2021-12-07T05:38:55.581726Z","shell.execute_reply.started":"2021-12-07T05:38:54.419199Z","shell.execute_reply":"2021-12-07T05:38:55.580906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"exchn = []\nfor conver in convers:\n    exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())\n\ndiag = {}\nfor line in lines:\n    diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]\n\n## delete\ndel(lines, convers, conver, line)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:55.583885Z","iopub.execute_input":"2021-12-07T05:38:55.584153Z","iopub.status.idle":"2021-12-07T05:38:56.29428Z","shell.execute_reply.started":"2021-12-07T05:38:55.584119Z","shell.execute_reply":"2021-12-07T05:38:56.293551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating List of Questions and Answers","metadata":{}},{"cell_type":"code","source":"questions = []\nanswers = []\n\nfor conver in exchn:\n    for i in range(len(conver) - 1):\n        questions.append(diag[conver[i]])\n        answers.append(diag[conver[i+1]])\ndel(diag, exchn, conver, i)\n\n# More preprocessing of QnA\n# Maximum Length of Questions= 13    \n\nsorted_ques = []\nsorted_ans = []\nfor i in range(len(questions)):\n    if len(questions[i]) < 13:\n        sorted_ques.append(questions[i])\n        sorted_ans.append(answers[i])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:56.295432Z","iopub.execute_input":"2021-12-07T05:38:56.295679Z","iopub.status.idle":"2021-12-07T05:38:56.580147Z","shell.execute_reply.started":"2021-12-07T05:38:56.295646Z","shell.execute_reply":"2021-12-07T05:38:56.579375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning of Dataset","metadata":{}},{"cell_type":"code","source":"def clean_text(txt):\n    txt = txt.lower()\n    txt = re.sub(r\"i'm\", \"i am\", txt)\n    txt = re.sub(r\"he's\", \"he is\", txt)\n    txt = re.sub(r\"she's\", \"she is\", txt)\n    txt = re.sub(r\"that's\", \"that is\", txt)\n    txt = re.sub(r\"what's\", \"what is\", txt)\n    txt = re.sub(r\"where's\", \"where is\", txt)\n    txt = re.sub(r\"\\'ll\", \" will\", txt)\n    txt = re.sub(r\"\\'ve\", \" have\", txt)\n    txt = re.sub(r\"\\'re\", \" are\", txt)\n    txt = re.sub(r\"\\'d\", \" would\", txt)\n    txt = re.sub(r\"won't\", \"will not\", txt)\n    txt = re.sub(r\"can't\", \"can not\", txt)\n    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n    return txt\n\nclean_ques = []\nclean_ans = []\n\nfor line in sorted_ques:\n    clean_ques.append(clean_text(line))\n    \nfor line in sorted_ans:\n    clean_ans.append(clean_text(line))\n\nfor i in range(len(clean_ans)):\n    clean_ans[i] = ' '.join(clean_ans[i].split()[:13])\n\ndel(answers, questions, line,sorted_ans, sorted_ques)\n\n\n# trimming\nclean_ans=clean_ans[:35000]\nclean_ques=clean_ques[:35000]","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:56.581374Z","iopub.execute_input":"2021-12-07T05:38:56.581618Z","iopub.status.idle":"2021-12-07T05:38:57.645157Z","shell.execute_reply.started":"2021-12-07T05:38:56.581585Z","shell.execute_reply":"2021-12-07T05:38:57.644385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Vocabulary","metadata":{}},{"cell_type":"code","source":"#  Count Occurences \nword2count = {}\n\nfor line in clean_ques:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n            \nfor line in clean_ans:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n            \ndel(word, line)\n\n\n# Remove less frequent Words by threshold frequency\nthresh = 5\nvocab = {}\nword_num = 0\nfor word, count in word2count.items():\n    if count >= thresh:\n        vocab[word] = word_num\n        word_num += 1\n        \n## delete\ndel(word2count, word, count, thresh,word_num)       \n   \n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:57.646606Z","iopub.execute_input":"2021-12-07T05:38:57.646896Z","iopub.status.idle":"2021-12-07T05:38:57.758687Z","shell.execute_reply.started":"2021-12-07T05:38:57.646862Z","shell.execute_reply":"2021-12-07T05:38:57.758081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding SOS and EOS","metadata":{}},{"cell_type":"code","source":"for i in range(len(clean_ans)):\n    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'\n    \ntokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\nx = len(vocab)\n\nfor token in tokens:\n    vocab[token] = x\n    x += 1\n    \nvocab['cameron'] = vocab['<PAD>']\nvocab['<PAD>'] = 0\n\ndel(x,token, tokens) \n\n# Inverse Answers Dictionary \ninv_vocab = {w:v for v, w in vocab.items()}\ndel(i)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:57.760543Z","iopub.execute_input":"2021-12-07T05:38:57.761039Z","iopub.status.idle":"2021-12-07T05:38:57.777682Z","shell.execute_reply.started":"2021-12-07T05:38:57.761003Z","shell.execute_reply":"2021-12-07T05:38:57.776964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Encoder and Decoder Inputs","metadata":{}},{"cell_type":"code","source":"encoder_inp = []\nfor line in clean_ques:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])\n        \n    encoder_inp.append(lst)\n    \ndecoder_inp = []\nfor line in clean_ans:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])        \n    decoder_inp.append(lst)\n\ndel(clean_ans, clean_ques, line, lst, word)\n\n# Padding the inputs for LSTM Model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nencoder_inp = pad_sequences(encoder_inp, 13, padding='post', truncating='post')\ndecoder_inp = pad_sequences(decoder_inp, 13, padding='post', truncating='post')\ndecoder_final_output = []\n\nfor i in decoder_inp:\n    decoder_final_output.append(i[1:]) \ndecoder_final_output = pad_sequences(decoder_final_output, 13, padding='post', truncating='post')\ndel(i)\n\n# Label Encoding\ndecoder_final_output = to_categorical(decoder_final_output, len(vocab))\nprint(decoder_final_output.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:57.778872Z","iopub.execute_input":"2021-12-07T05:38:57.780135Z","iopub.status.idle":"2021-12-07T05:38:59.299512Z","shell.execute_reply.started":"2021-12-07T05:38:57.780096Z","shell.execute_reply":"2021-12-07T05:38:59.297949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Encoding  Model Using LSTM","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n\nenc_inp = Input(shape=(13, ))\ndec_inp = Input(shape=(13, ))\n\nVOCAB_SIZE = len(vocab)\nembed = Embedding(VOCAB_SIZE+1, output_dim=50, \n                  input_length=13,\n                  trainable=True                  \n                  )\n\nenc_embed = embed(enc_inp)\nenc_lstm = LSTM(400, return_sequences=True, return_state=True)\nenc_op, h, c = enc_lstm(enc_embed)\nenc_states = [h, c]\n\ndec_embed = embed(dec_inp)\ndec_lstm = LSTM(400, return_sequences=True, return_state=True)\ndec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n\ndense = Dense(VOCAB_SIZE, activation='softmax')\n\ndense_op = dense(dec_op)\n#model = tf.keras.models.load_model(\"../input/models/model.h5\")\nmodel = Model([enc_inp, dec_inp], dense_op)\n\nmodel.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer='adam')\n\nmodel.fit([encoder_inp, decoder_inp],decoder_final_output,epochs=300)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:38:59.302607Z","iopub.execute_input":"2021-12-07T05:38:59.30286Z","iopub.status.idle":"2021-12-07T06:36:49.591368Z","shell.execute_reply.started":"2021-12-07T05:38:59.302831Z","shell.execute_reply":"2021-12-07T06:36:49.59063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.save(\"dec_model.h5\")\n#model.save(\"model.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:36:49.592498Z","iopub.execute_input":"2021-12-07T06:36:49.59274Z","iopub.status.idle":"2021-12-07T06:36:49.596502Z","shell.execute_reply.started":"2021-12-07T06:36:49.592707Z","shell.execute_reply":"2021-12-07T06:36:49.595531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Decoding Model Using LSTM","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nimport tensorflow as tf\n#model = tf.keras.models.load_model(\"../input/models/model.h5\")\n\nenc_model = Model([enc_inp], enc_states)\n\n# decoder Model\ndecoder_state_input_h = Input(shape=(400,))\ndecoder_state_input_c = Input(shape=(400,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_outputs, state_h, state_c = dec_lstm(dec_embed , \n                                    initial_state=decoder_states_inputs)\n\ndecoder_states = [state_h, state_c]\n#dec_model = tf.keras.models.load_model(\"../input/models/dec_model.h5\")\ndec_model = Model([dec_inp]+ decoder_states_inputs,[decoder_outputs]+ decoder_states)\n\ndec_model.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:36:49.597868Z","iopub.execute_input":"2021-12-07T06:36:49.598309Z","iopub.status.idle":"2021-12-07T06:36:49.796304Z","shell.execute_reply.started":"2021-12-07T06:36:49.598219Z","shell.execute_reply":"2021-12-07T06:36:49.795667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference Model for Frontend","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nconversations = open('../input/testgg/Test.txt', encoding='utf-8',\n             errors='ignore').read().split(\"\\nq\")\nconversations.pop()\nprint(\"##########################################\")\nprint(\"#       start chatting ver. 1.0          #\")\nprint(\"##########################################\")\nprint(\"\")\nLines = []\ni=0\nCHATBOT_ACTIVITY_VOLUME = [] #Measuring a chatbot’s Activity Volume means evaluating the number of interactions, \n#from the time a user asks a simple question until a constructive dialogue takes place.\nNON_RESPONSE_RATE = [] #This metric measures the number of times your chatbot fails to respond to a question. \n#Such failure may be the result of a lack of content or of your bot’s difficulty in comprehending user inquiries.\nRESPONSE_RATE = [] #This is a concrete indicator that will tell you the number of questions your chatbot has answered.\nfor i in range (len(conversations)):\n    CHATBOT_ACTIVITY_VOLUME.append(0)\n    NON_RESPONSE_RATE.append(0)\n    RESPONSE_RATE.append(0)\ni=0\nfor i in range (len(conversations)):\n print(\"Conversation\",i+1,\": \") \n print(\"\")\n k=0\n l=0\n lines = conversations[i].split('\\n')\n #lines.pop()\n if i!=0:   \n  lines.pop(0)\n for j in range (len(lines)):\n    #prepro1 = \"\"\n    prepro1 = lines[j]\n    #while prepro1 != 'q':\n    l=0\n    CHATBOT_ACTIVITY_VOLUME[i]+=1\n    NON_RESPONSE_RATE[i] = k*100/CHATBOT_ACTIVITY_VOLUME[i]\n    RESPONSE_RATE[i] = 100 - NON_RESPONSE_RATE[i]\n    #prepro1  = input(\"you : \")\n    \n    prepro1 = clean_text(prepro1)\n    prepro = [prepro1]\n\n    txt = []\n    for x in prepro:\n        \n        lst = []\n        for y in x.split():\n           \n            try:\n                lst.append(vocab[y])\n            \n            except:\n                lst.append(vocab['<OUT>'])\n                l=1\n        txt.append(lst)\n\n    txt = pad_sequences(txt, 13, padding='post')\n\n    stat = enc_model.predict( txt )\n\n    empty_target_seq = np.zeros( ( 1 , 1) )\n\n    empty_target_seq[0, 0] = vocab['<SOS>']\n\n    stop_condition = False\n    decoded_translation = ''\n\n    while not stop_condition :\n\n        dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )\n        decoder_concat_input = dense(dec_outputs)\n\n        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n      \n        sampled_word = inv_vocab[sampled_word_index] + ' '\n\n        if sampled_word != '<EOS> ':\n            decoded_translation += sampled_word  \n\n        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n            stop_condition = True \n\n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        stat = [h, c]  \n    print(\"you : \",prepro1)\n    print(\"chatbot attention : \", decoded_translation )\n    #print(\"==============================================\")\n    Lines.append(prepro1)\n    if l==1:\n        k+=1\n print(\"==============================================\")\n print(\"\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:36:49.797701Z","iopub.execute_input":"2021-12-07T06:36:49.79809Z","iopub.status.idle":"2021-12-07T06:37:46.349067Z","shell.execute_reply.started":"2021-12-07T06:36:49.798056Z","shell.execute_reply":"2021-12-07T06:37:46.348338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jargon = open('../input/testgg/Jargon.txt', encoding='utf-8',\n             errors='ignore').read().split(\"\\n\")\nprint(\"Jargon used is : \",jargon)\nCOMPREHENSION_LEVEL = [] #Your chatbot will indicate its overall \n#comprehension of user inquiries.\nfor j in range(len(jargon)):\n    COMPREHENSION_LEVEL.append(0)\nfor j in range (len(jargon)):\n    #prepro1 = \"\"\n    prepro1 = jargon[j]\n    #while prepro1 != 'q':\n    l=100\n    \n    prepro1 = clean_text(prepro1)\n    prepro = [prepro1]\n\n    txt = []\n    for x in prepro:\n        \n        lst = []\n        for y in x.split():\n           \n            try:\n                lst.append(vocab[y])\n            \n            except:\n                lst.append(vocab['<OUT>'])\n                l=0\n        txt.append(lst)\n\n    txt = pad_sequences(txt, 13, padding='post')\n\n    stat = enc_model.predict( txt )\n\n    empty_target_seq = np.zeros( ( 1 , 1) )\n\n    empty_target_seq[0, 0] = vocab['<SOS>']\n\n    stop_condition = False\n    decoded_translation = ''\n\n    while not stop_condition :\n\n        dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )\n        decoder_concat_input = dense(dec_outputs)\n\n        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n      \n        sampled_word = inv_vocab[sampled_word_index] + ' '\n\n        if sampled_word != '<EOS> ':\n            decoded_translation += sampled_word  \n\n        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n            stop_condition = True \n\n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        stat = [h, c]  \n    COMPREHENSION_LEVEL[j] = l ","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:46.350451Z","iopub.execute_input":"2021-12-07T06:37:46.350714Z","iopub.status.idle":"2021-12-07T06:37:54.243515Z","shell.execute_reply.started":"2021-12-07T06:37:46.350678Z","shell.execute_reply":"2021-12-07T06:37:54.242805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"CHATBOT_ACTIVITY_VOLUME = \", CHATBOT_ACTIVITY_VOLUME)\nprint(\"\")\nprint(\"RESPONSE_RATE = \", RESPONSE_RATE)\nprint(\"\")\nprint(\"NON_RESPONSE_RATE = \", NON_RESPONSE_RATE)\nprint(\"\")\nINTERACTION_RATE = sum(CHATBOT_ACTIVITY_VOLUME)/len(CHATBOT_ACTIVITY_VOLUME)\nprint(\"\")\nprint(\"INERACTION RATE = \",INTERACTION_RATE)\nprint(\"\")\nprint(\"COMPREHENSION LEVEL = \",COMPREHENSION_LEVEL)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:54.24489Z","iopub.execute_input":"2021-12-07T06:37:54.245151Z","iopub.status.idle":"2021-12-07T06:37:54.254915Z","shell.execute_reply.started":"2021-12-07T06:37:54.245115Z","shell.execute_reply":"2021-12-07T06:37:54.254044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lcs(S,T):\n    S.upper()\n    T.upper()\n    m = len(S)\n    n = len(T)\n    counter = [[0]*(n+1) for x in range(m+1)]\n    longest = 0\n    lcs_set = set()\n    for i in range(m):\n        for j in range(n):\n            if S[i] == T[j]:\n                c = counter[i][j] + 1\n                counter[i+1][j+1] = c\n                if c > longest:\n                    lcs_set = set()\n                    longest = c\n                    lcs_set.add(S[i-c+1:i+1])\n                #elif c == longest:\n                    #lcs_set.add(S[i-c+1:i+1])\n\n    return lcs_set","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:54.256271Z","iopub.execute_input":"2021-12-07T06:37:54.256589Z","iopub.status.idle":"2021-12-07T06:37:54.265575Z","shell.execute_reply.started":"2021-12-07T06:37:54.25655Z","shell.execute_reply":"2021-12-07T06:37:54.264818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MFAQ = [] #Most frequently asked questions\nfor i in range(len(Lines)):\n    Lines[i] = Lines[i].strip()\nLines = list(dict.fromkeys(Lines))\nfor i in range(len(Lines)):\n    MFAQ.append(0)\nfor j in range (len(Lines)):\n    for k in range(len(Lines)):\n      if(j!=k):\n        omg = len(repr(lcs(Lines[j].upper(),Lines[k].upper())))\n        omg = omg - 4\n        if(omg*100/len(Lines[j])>51):\n            MFAQ[j] = MFAQ[j] + 1\n        #if(omg*100/len(Lines[k])>51):\n            #MFAQ[k] = MFAQ[k] + 1\nn = len(MFAQ)\n \n    # Traverse through all array elements\nfor i in range(n):\n \n        # Last i elements are already in place\n    for j in range(0, n-i-1):\n \n            # traverse the array from 0 to n-i-1\n            # Swap if the element found is greater\n            # than the next element\n        if MFAQ[j] < MFAQ[j+1] :\n                MFAQ[j], MFAQ[j+1] = MFAQ[j+1], MFAQ[j]\n                Lines[j], Lines[j+1] = Lines[j+1], Lines[j]\nprint(\"Most frequently asked questions are : \")\nprint(Lines[0:20])\nmdf = pd.DataFrame(list(zip(Lines,MFAQ)),\n               columns =['Lines', 'Frequency'])","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:54.266836Z","iopub.execute_input":"2021-12-07T06:37:54.267232Z","iopub.status.idle":"2021-12-07T06:37:56.235932Z","shell.execute_reply.started":"2021-12-07T06:37:54.267197Z","shell.execute_reply":"2021-12-07T06:37:56.235175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mdf","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:56.237039Z","iopub.execute_input":"2021-12-07T06:37:56.237873Z","iopub.status.idle":"2021-12-07T06:37:56.254813Z","shell.execute_reply.started":"2021-12-07T06:37:56.237814Z","shell.execute_reply":"2021-12-07T06:37:56.253974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count plot on two categorical variable\n#sns.countplot(x ='Lines',y='Frequency', hue = 'Frequency', data = mdf)\n \n# Show the plot\n#plt.show()\nmdf.plot.bar(x='Lines',y='Frequency')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:56.256022Z","iopub.execute_input":"2021-12-07T06:37:56.256341Z","iopub.status.idle":"2021-12-07T06:37:59.55833Z","shell.execute_reply.started":"2021-12-07T06:37:56.256299Z","shell.execute_reply":"2021-12-07T06:37:59.557633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(list(zip(CHATBOT_ACTIVITY_VOLUME,RESPONSE_RATE,COMPREHENSION_LEVEL)),\n               columns =['CHATBOT_ACTIVITY_VOLUME', 'RESPONSE_RATE', 'COMPREHENSION_LEVEL' ])\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:59.559259Z","iopub.execute_input":"2021-12-07T06:37:59.559476Z","iopub.status.idle":"2021-12-07T06:37:59.576467Z","shell.execute_reply.started":"2021-12-07T06:37:59.559444Z","shell.execute_reply":"2021-12-07T06:37:59.575648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:59.577696Z","iopub.execute_input":"2021-12-07T06:37:59.577962Z","iopub.status.idle":"2021-12-07T06:37:59.584797Z","shell.execute_reply.started":"2021-12-07T06:37:59.577928Z","shell.execute_reply":"2021-12-07T06:37:59.584142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:59.586056Z","iopub.execute_input":"2021-12-07T06:37:59.586327Z","iopub.status.idle":"2021-12-07T06:37:59.607234Z","shell.execute_reply.started":"2021-12-07T06:37:59.586278Z","shell.execute_reply":"2021-12-07T06:37:59.606623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.hist('CHATBOT_ACTIVITY_VOLUME')\ndf.hist('RESPONSE_RATE')\ndf.hist('COMPREHENSION_LEVEL')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:37:59.60849Z","iopub.execute_input":"2021-12-07T06:37:59.608724Z","iopub.status.idle":"2021-12-07T06:38:00.192911Z","shell.execute_reply.started":"2021-12-07T06:37:59.608693Z","shell.execute_reply":"2021-12-07T06:38:00.192211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 10)) # Set the figure size\nsns.heatmap(df.corr(), annot=True) # Print the heatmap","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:38:00.194189Z","iopub.execute_input":"2021-12-07T06:38:00.194429Z","iopub.status.idle":"2021-12-07T06:38:00.499566Z","shell.execute_reply.started":"2021-12-07T06:38:00.194395Z","shell.execute_reply":"2021-12-07T06:38:00.49896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # **Live questions**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nfrom keras.preprocessing.sequence import pad_sequences\nprint(\"##########################################\")\nprint(\"#       start chatting ver. 1.0          #\")\nprint(\"##########################################\")\n\n\nprepro1 = \"\"\nwhile prepro1 != 'q':\n    prepro1  = input(\"you : \")\n    \n    prepro1 = clean_text(prepro1)\n    prepro = [prepro1]\n\n    txt = []\n    for x in prepro:\n        \n        lst = []\n        for y in x.split():\n           \n            try:\n                lst.append(vocab[y])\n            \n            except:\n                lst.append(vocab['<OUT>'])\n        txt.append(lst)\n\n    txt = pad_sequences(txt, 13, padding='post')\n\n    stat = enc_model.predict( txt )\n\n    empty_target_seq = np.zeros( ( 1 , 1) )\n\n    empty_target_seq[0, 0] = vocab['<SOS>']\n\n    stop_condition = False\n    decoded_translation = ''\n\n    while not stop_condition :\n\n        dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )\n        decoder_concat_input = dense(dec_outputs)\n\n        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n      \n        sampled_word = inv_vocab[sampled_word_index] + ' '\n\n        if sampled_word != '<EOS> ':\n            decoded_translation += sampled_word  \n\n        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n            stop_condition = True \n\n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        stat = [h, c]  \n\n    print(\"chatbot attention : \", decoded_translation )\n    print(\"==============================================\")  ","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:38:00.500883Z","iopub.execute_input":"2021-12-07T06:38:00.501133Z"},"trusted":true},"execution_count":null,"outputs":[]}]}